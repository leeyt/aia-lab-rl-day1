{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 簡化版的deep q learning： 僅把q-table換成類神經網路\n",
    "\n",
    "在 Reinforcement Learning 中，我們都利用 $Q_{target}$（把它當作真實的Q值) 來更新神經網路的 weights。\n",
    "\n",
    "公式：\n",
    "\n",
    "$$ Q_{target} = Q(s) + \\alpha \\cdot (R(s, a) + Q(s_{next}) \\cdot \\gamma - Q(s)) $$\n",
    "\n",
    "($s_{next}$ 代表下一步的狀態，下一步的狀態有很多種可能，我們這裡選擇的 $s_{next}$ 是能得到最大Q的狀態，這種方法是比較 aggressive 的方法，還有另外一種是SARSA有興趣可以自尋搜尋一下；$\\alpha$ 這邊我們設定為1）\n",
    "\n",
    "因此公式就變成 \n",
    "\n",
    "$$ Q_{target} = R(s, a) + max(Q(s_{next}, a)) \\cdot \\gamma $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions,             # 動作的維度，例如上下左右就有四維\n",
    "        n_states,              # 用來描述狀態的維度，例如馬力歐在平面上就是二維\n",
    "        gamma = 0.9,           # 遠見程度，值越大越以未來收益為重\n",
    "        epsilon = 0.9,         # 保守程度，越大就越容易用Q值大小來採取行動；越小則越容易產生隨機行動\n",
    "        learning_rate = 0.001  # 神經網路的更新率\n",
    "    ):\n",
    "        # 變數給初始值\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        tf.reset_default_graph() ## 重新 build graph 需要跑這行\n",
    "        self.sess = tf.Session() ## 宣告session\n",
    "        \n",
    "        # 輸入 current state\n",
    "        self.state_input = tf.placeholder(shape=[None, self.n_states], name='input', dtype=tf.float32)\n",
    "        \n",
    "        # q_target = R(s, action) + Q(s_)*Gamma \n",
    "        self.q_target = tf.placeholder(shape=[None, self.n_actions], name='q_target', dtype=tf.float32)\n",
    "        \n",
    "        # 搭建神經網路\n",
    "        with tf.variable_scope('Q_table'):\n",
    "            self.q_eval = self.build_network('net_eval') \n",
    "        \n",
    "        # 管理神經網路的 parameters\n",
    "        self.Qnet_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Q_table/net_eval')\n",
    "        \n",
    "        # 計算 q_target 和 q_eval 的 MSE 來更新神經網路的參數\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        self.train = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, var_list=self.Qnet_eval_params)\n",
    "        \n",
    "        # 將神經網路初始化\n",
    "        self.sess.run(tf.global_variables_initializer()) \n",
    "            \n",
    "    def build_network(self, scope): \n",
    "        with tf.variable_scope(scope):\n",
    "            x_h1 = tf.layers.dense(inputs=self.state_input, units=5, activation=tf.nn.tanh)\n",
    "            x_h2 = tf.layers.dense(inputs=x_h1, units=5, activation=tf.nn.tanh)\n",
    "            \n",
    "        # 輸出 \"不同動作\" 對應的Q值\n",
    "        return tf.layers.dense(inputs=x_h2, units=self.n_actions)\n",
    "            \n",
    "    def choose_action(self, current_state):\n",
    "        \"\"\"\n",
    "        利用 epsilon 來控制探索的隨機程度，通常探索初期會給比較小的 epsilon 增加行為的隨機程度，\n",
    "        然後隨著遊戲的進行慢慢增加 epsilon。不過由於這裡的遊戲較簡單，就不做此設定。\n",
    "        \"\"\"\n",
    "        if np.random.uniform() < self.epsilon: \n",
    "            # 選擇產生估計Q值較大的行動\n",
    "            q_eval = self.sess.run(self.q_eval, feed_dict={self.state_input: current_state[np.newaxis, :]})\n",
    "            self.action = np.argmax(q_eval)\n",
    "        else:\n",
    "            # 採取隨機行動\n",
    "            self.action = np.random.randint(0, self.n_actions)\n",
    "\n",
    "        return self.action\n",
    "    \n",
    "    def learn(self, current_state, reward, next_state):\n",
    "        # 算出實際 Q 值並用此更新神經網路參數\n",
    "        q_eval = self.sess.run(self.q_eval, feed_dict={self.state_input: current_state[np.newaxis, :]})\n",
    "        q_eval_next = self.sess.run(self.q_eval, feed_dict={self.state_input: next_state[np.newaxis, :]})\n",
    "        q_target = q_eval.copy()\n",
    "        q_target[:, self.action] = reward + self.gamma*q_eval_next.max()\n",
    "        _, self.cost = self.sess.run(\n",
    "            [self.train, self.loss],\n",
    "            feed_dict={self.state_input: current_state[np.newaxis, :], self.q_target: q_target})\n",
    "\n",
    "    def model_save(self, model_name):        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    \n",
    "    def model_restore(self, model_name):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, \"saved_models/{}.ckpt\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讓RL Agent開始與環境互動吧～\n",
    "\n",
    "**提示**\n",
    "\n",
    "在/Users/Yourname/anaconda3/lib/python3.6/site-packages/gym/envs底下可以找到Gym AI底下所有遊戲的文件，其中__init__.py定義了呼叫各個遊戲的名稱，例如moutain car你就得用gym.make(‘MountainCar-v0’)，另外和遊戲相關的py檔在envs/classic_control的資料夾內。我們接下來要玩的是離散版本的不是連續版的喔～，另外如果您找不到的話我們也將檔案拉出來放在gym document供大家參考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Total Reward:-6812.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\"\"\"\n",
    "執行gym ai的遊戲時也請加下面這兩行\n",
    "\"\"\"\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "RL = QLearning(n_actions = 3, \n",
    "               n_states = 2,\n",
    "               gamma = 0.9,\n",
    "               epsilon = 0.8,\n",
    "               learning_rate = 0.01\n",
    "               )\n",
    "\n",
    "reward_result = []\n",
    "\n",
    "# 初始化環境並取得起始狀態\n",
    "total_reward = 0\n",
    "current_state = env.reset()\n",
    "\n",
    "while True:\n",
    "    # 產生環境視窗\n",
    "    env.render()\n",
    "\n",
    "    # 依據目前狀態，選擇下一步行動\n",
    "    action = RL.choose_action(current_state)\n",
    "    \n",
    "    \"\"\"\n",
    "    Gym ai 的遊戲step都會output 4個值，分別為下一狀態、\n",
    "    獎勵、回合結束與否和info，不過info我們用不到因此不用管它\n",
    "    \"\"\"\n",
    "    \n",
    "    # 採取行動，從環境中取得更新狀態、獎勵、遊戲是否結束\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    total_reward+= reward\n",
    "\n",
    "    RL.learn(current_state, reward, next_state)\n",
    "\n",
    "    # break while loop when end of this episode\n",
    "    if done:\n",
    "        print('Total Reward:{}'.format(total_reward))\n",
    "        reward_result.append(total_reward)\n",
    "        break\n",
    "    # swap state\n",
    "    current_state = next_state  \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
