{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定義 DeepQLearning\n",
    "\n",
    "請大家將 class 中 function 的內容完成。投影片上應該也有答案～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        n_states, \n",
    "        gamma = 0.9,             # 遠見程度\n",
    "        epsilon = 0.9,           # 保守程度，越大就越容易用Q值大小來採取行動；越小則越容易產生隨機行動\n",
    "        epsilon_increase = None,\n",
    "        learning_rate = 0.001,   # 神經網路的更新率\n",
    "        memory_size = 50,        # 記憶庫大小，用來記錄過往經驗\n",
    "        batch_size = 32,\n",
    "        neuron_num = 10\n",
    "    ):\n",
    "        # 初始化變數值\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_max = epsilon \n",
    "        self.epsilon_increase = epsilon_increase \n",
    "        self.epsilon = 0 if epsilon_increase is not None else epsilon\n",
    "        self.lr = learning_rate\n",
    "        self.memory_size = memory_size\n",
    "        self.memory_counter = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.neuron_num = neuron_num\n",
    "        \n",
    "        # 初始化記憶庫\n",
    "        self.memory = np.zeros((self.memory_size, n_states * 2 + 2))\n",
    "        \"\"\"\n",
    "        memory 用來記錄探索環境所蒐集到的資料用來更新神經網路，從公式 \n",
    "        \n",
    "            Q_target = R(s,a) + max(Q(s_,a))*Gamma\n",
    "        \n",
    "        得知我們需要存取的資訊有 s(current state), s_(next state), r(reward), a(action) \n",
    "        \n",
    "        因此總共需要 2*n_states+2 個維度\n",
    "        \"\"\"\n",
    "        \n",
    "        tf.reset_default_graph() ## 重新 build graph 需要跑這行\n",
    "        self.sess = tf.Session() ## 宣告 session\n",
    "        \n",
    "        # 輸入current state\n",
    "        self.state_input = tf.placeholder(shape=[None, self.n_states], name='input', dtype = tf.float32)\n",
    "        # q_target = R(s, action) + Q(s_)*Gamma \n",
    "        self.q_target = tf.placeholder(shape=[None, self.n_actions], name='q_target', dtype = tf.float32)\n",
    "        \n",
    "        # 搭建神經網路\n",
    "        with tf.variable_scope('Q_table'):\n",
    "            self.q_eval = self.build_network(self.neuron_num, trainable=True, scope = 'net_eval') \n",
    "            self.q_next = self.build_network(self.neuron_num, trainable=False, scope = 'net_target') ##### 利用另外一個參數較舊的神經網路來給出下一步的q值並計算q_target\n",
    "        \n",
    "        # 管理神經網路的 parameters\n",
    "        self.Qnet_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Q_table/net_eval')\n",
    "        self.Qnet_target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Q_table/net_target') #####\n",
    "        \n",
    "        # 更新舊神經網路的參數\n",
    "        self.replace_net_params = [tf.assign(old, new) for old, new in zip(self.Qnet_target_params, self.Qnet_eval_params)]\n",
    "        \n",
    "        # 計算 q_target 和 q_eval 的 MSE 來更新神經網路的參數\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        self.train = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, var_list=self.Qnet_eval_params)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer()) # 將神經網路初始化\n",
    "    \n",
    "    def write_memory(self, current_state, next_state, reward, action):\n",
    "        env_info = np.concatenate([ current_state, next_state, [reward], [action] ])\n",
    "        save_index = self.memory_counter % self.memory_size\n",
    "        self.memory[save_index, :] = env_info\n",
    "        \n",
    "        self.memory_counter += 1\n",
    "        if self.memory_counter % self.memory_size == 0:\n",
    "            self.memory_counter = self.memory_size\n",
    "    \n",
    "    def build_network(self, neuron_num, trainable, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0, 0.3)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "            \n",
    "            x_h1 = tf.layers.dense(inputs=self.state_input, units=neuron_num, activation=tf.nn.tanh, \n",
    "                                   kernel_initializer=init_w, bias_initializer=init_b, trainable=trainable)\n",
    "            output = tf.layers.dense(inputs=x_h1, units=self.n_actions, kernel_initializer=init_w, \n",
    "                                     bias_initializer=init_b, trainable=trainable)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "    def choose_action(self, current_state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # 選擇讓 Q 值較大的行動\n",
    "            q_eval = self.sess.run(self.q_eval, feed_dict={ self.state_input: current_state[np.newaxis, :] })\n",
    "            self.action = np.argmax(q_eval)\n",
    "        else:\n",
    "            # 採取隨機行動\n",
    "            self.action = np.random.randint(0, self.n_actions)\n",
    "            \n",
    "        return self.action\n",
    "        \n",
    "    def learn(self, current_state, reward, next_state, update_params):\n",
    "        if update_params:\n",
    "            self.sess.run(self.replace_net_params)\n",
    "            \n",
    "        if self.memory_counter > self.memory_size:\n",
    "            # 記憶庫已經裝滿\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            # 記憶庫還沒裝滿\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        \n",
    "        past_experience = self.memory[sample_index, :]\n",
    "        current_state = past_experience[:, 0:self.n_states]\n",
    "        next_state = past_experience[:, self.n_states:self.n_states*2]\n",
    "        reward = past_experience[:, self.n_states*2]\n",
    "        action = past_experience[:, self.n_states*2+1].astype(int)\n",
    "\n",
    "        index = np.arange(self.batch_size)\n",
    "        q_eval = self.sess.run(self.q_eval, feed_dict={ self.state_input: current_state })\n",
    "        q_next = self.sess.run(self.q_next, feed_dict={ self.state_input: next_state })\n",
    "        \n",
    "        q_target = q_eval.copy()\n",
    "        q_target[index, action] = reward + self.gamma*np.max(q_next, axis=1)\n",
    "        _, self.cost = self.sess.run([ self.train, self.loss ], \n",
    "                                     feed_dict={ self.state_input: current_state, self.q_target: q_target })\n",
    "        \n",
    "        # Increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increase if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "    \n",
    "    def model_save(self, model_name):    \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    \n",
    "    def model_restore(self, model_name):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, \"saved_models/{}.ckpt\".format(model_name))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定義 Training 的環境和步驟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_model, model_name):\n",
    "    step_record = []\n",
    "    learn_count = 0\n",
    "    update_count = 0\n",
    "    for episode in range(100):\n",
    "        # 初始化環境並取得起始的狀態\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            # 產生環境視窗\n",
    "            env.render()\n",
    "\n",
    "            # 根據現在的狀態選擇動作\n",
    "            action = RL.choose_action(current_state)\n",
    "\n",
    "            # 產生動作和環境互動後產生下一個狀態、獎勵值及遊戲是否結束\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # 調整reward\n",
    "            \"\"\"\n",
    "            引導agent將車子推得越遠越好、速度越快越好\n",
    "            position的原點在-0.5因此這邊將0.5加回來\n",
    "            \"\"\"\n",
    "            position, velocity = next_state\n",
    "            reward = abs(position + 0.5) + abs(velocity)*15 + reward\n",
    "            \"\"\"\n",
    "            表現較差的方法，但是在一般問題中我們常常是不知道怎麼引導agent\n",
    "            達到目標的，因此這樣的方法相對直覺。\n",
    "            #if done == True:\n",
    "            #    reward = 100\n",
    "            #else:\n",
    "            #    reward = -1\n",
    "            \"\"\"\n",
    "\n",
    "            # 將資訊存至記憶體中以便進行經驗回顧 (experience replay)\n",
    "            RL.write_memory(current_state, next_state, reward, action)\n",
    "            \n",
    "            # 進行學習\n",
    "            if learn_count <= 1000:\n",
    "                learn_count += 1\n",
    "            else:\n",
    "                # 進行參數更新\n",
    "                if update_count == 300:\n",
    "                    update = True\n",
    "                    update_count =0\n",
    "                else:\n",
    "                    update = False\n",
    "                    update_count += 1\n",
    "                \n",
    "                RL.learn(current_state, reward, next_state, update)\n",
    "            \n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                mean_reward = total_reward / step\n",
    "                print('episode:{} steps:{} mean reward:{} epsilon:{}'.format(episode, step, \\\n",
    "                        mean_reward, RL.epsilon))\n",
    "                step_record.append(step)\n",
    "                break\n",
    "            \n",
    "            current_state = next_state\n",
    "            step += 1\n",
    "\n",
    "    # end of game\n",
    "    \n",
    "    if save_model:\n",
    "        RL.model_save(model_name)\n",
    "    print('game over')\n",
    "    env.close()\n",
    "    \n",
    "    return step_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode:0 steps:3260 mean reward:0.0 epsilon:0.95\n",
      "episode:1 steps:194 mean reward:0.0 epsilon:0.95\n",
      "episode:2 steps:195 mean reward:0.0 epsilon:0.95\n",
      "episode:3 steps:199 mean reward:0.0 epsilon:0.95\n",
      "episode:4 steps:235 mean reward:0.0 epsilon:0.95\n",
      "episode:5 steps:228 mean reward:0.0 epsilon:0.95\n",
      "episode:6 steps:202 mean reward:0.0 epsilon:0.95\n",
      "episode:7 steps:229 mean reward:0.0 epsilon:0.95\n",
      "episode:8 steps:156 mean reward:0.0 epsilon:0.95\n",
      "episode:9 steps:140 mean reward:0.0 epsilon:0.95\n",
      "episode:10 steps:149 mean reward:0.0 epsilon:0.95\n",
      "episode:11 steps:165 mean reward:0.0 epsilon:0.95\n",
      "episode:12 steps:250 mean reward:0.0 epsilon:0.95\n",
      "episode:13 steps:119 mean reward:0.0 epsilon:0.95\n",
      "episode:14 steps:157 mean reward:0.0 epsilon:0.95\n",
      "episode:15 steps:149 mean reward:0.0 epsilon:0.95\n",
      "episode:16 steps:152 mean reward:0.0 epsilon:0.95\n",
      "episode:17 steps:166 mean reward:0.0 epsilon:0.95\n",
      "episode:18 steps:156 mean reward:0.0 epsilon:0.95\n",
      "episode:19 steps:149 mean reward:0.0 epsilon:0.95\n",
      "episode:20 steps:153 mean reward:0.0 epsilon:0.95\n",
      "episode:21 steps:124 mean reward:0.0 epsilon:0.95\n",
      "episode:22 steps:191 mean reward:0.0 epsilon:0.95\n",
      "episode:23 steps:161 mean reward:0.0 epsilon:0.95\n",
      "episode:24 steps:126 mean reward:0.0 epsilon:0.95\n",
      "episode:25 steps:120 mean reward:0.0 epsilon:0.95\n",
      "episode:26 steps:118 mean reward:0.0 epsilon:0.95\n",
      "episode:27 steps:157 mean reward:0.0 epsilon:0.95\n",
      "episode:28 steps:121 mean reward:0.0 epsilon:0.95\n",
      "episode:29 steps:148 mean reward:0.0 epsilon:0.95\n",
      "episode:30 steps:157 mean reward:0.0 epsilon:0.95\n",
      "episode:31 steps:151 mean reward:0.0 epsilon:0.95\n",
      "episode:32 steps:118 mean reward:0.0 epsilon:0.95\n",
      "episode:33 steps:123 mean reward:0.0 epsilon:0.95\n",
      "episode:34 steps:144 mean reward:0.0 epsilon:0.95\n",
      "episode:35 steps:164 mean reward:0.0 epsilon:0.95\n",
      "episode:36 steps:149 mean reward:0.0 epsilon:0.95\n",
      "episode:37 steps:133 mean reward:0.0 epsilon:0.95\n",
      "episode:38 steps:149 mean reward:0.0 epsilon:0.95\n",
      "episode:39 steps:149 mean reward:0.0 epsilon:0.95\n",
      "episode:40 steps:138 mean reward:0.0 epsilon:0.95\n",
      "episode:41 steps:144 mean reward:0.0 epsilon:0.95\n",
      "episode:42 steps:138 mean reward:0.0 epsilon:0.95\n",
      "episode:43 steps:143 mean reward:0.0 epsilon:0.95\n",
      "episode:44 steps:146 mean reward:0.0 epsilon:0.95\n",
      "episode:45 steps:142 mean reward:0.0 epsilon:0.95\n",
      "episode:46 steps:114 mean reward:0.0 epsilon:0.95\n",
      "episode:47 steps:143 mean reward:0.0 epsilon:0.95\n",
      "episode:48 steps:143 mean reward:0.0 epsilon:0.95\n",
      "episode:49 steps:151 mean reward:0.0 epsilon:0.95\n",
      "episode:50 steps:148 mean reward:0.0 epsilon:0.95\n",
      "episode:51 steps:147 mean reward:0.0 epsilon:0.95\n",
      "episode:52 steps:150 mean reward:0.0 epsilon:0.95\n",
      "episode:53 steps:148 mean reward:0.0 epsilon:0.95\n",
      "episode:54 steps:151 mean reward:0.0 epsilon:0.95\n",
      "episode:55 steps:215 mean reward:0.0 epsilon:0.95\n",
      "episode:56 steps:146 mean reward:0.0 epsilon:0.95\n",
      "episode:57 steps:148 mean reward:0.0 epsilon:0.95\n",
      "episode:58 steps:152 mean reward:0.0 epsilon:0.95\n",
      "episode:59 steps:150 mean reward:0.0 epsilon:0.95\n",
      "episode:60 steps:151 mean reward:0.0 epsilon:0.95\n",
      "episode:61 steps:156 mean reward:0.0 epsilon:0.95\n",
      "episode:62 steps:151 mean reward:0.0 epsilon:0.95\n",
      "episode:63 steps:154 mean reward:0.0 epsilon:0.95\n",
      "episode:64 steps:148 mean reward:0.0 epsilon:0.95\n",
      "episode:65 steps:150 mean reward:0.0 epsilon:0.95\n",
      "episode:66 steps:164 mean reward:0.0 epsilon:0.95\n",
      "episode:67 steps:163 mean reward:0.0 epsilon:0.95\n",
      "episode:68 steps:170 mean reward:0.0 epsilon:0.95\n",
      "episode:69 steps:236 mean reward:0.0 epsilon:0.95\n",
      "episode:70 steps:165 mean reward:0.0 epsilon:0.95\n",
      "episode:71 steps:158 mean reward:0.0 epsilon:0.95\n",
      "episode:72 steps:155 mean reward:0.0 epsilon:0.95\n",
      "episode:73 steps:264 mean reward:0.0 epsilon:0.95\n",
      "episode:74 steps:217 mean reward:0.0 epsilon:0.95\n",
      "episode:75 steps:129 mean reward:0.0 epsilon:0.95\n",
      "episode:76 steps:119 mean reward:0.0 epsilon:0.95\n",
      "episode:77 steps:128 mean reward:0.0 epsilon:0.95\n",
      "episode:78 steps:120 mean reward:0.0 epsilon:0.95\n",
      "episode:79 steps:209 mean reward:0.0 epsilon:0.95\n",
      "episode:80 steps:129 mean reward:0.0 epsilon:0.95\n",
      "episode:81 steps:156 mean reward:0.0 epsilon:0.95\n",
      "episode:82 steps:125 mean reward:0.0 epsilon:0.95\n",
      "episode:83 steps:124 mean reward:0.0 epsilon:0.95\n",
      "episode:84 steps:128 mean reward:0.0 epsilon:0.95\n",
      "episode:85 steps:121 mean reward:0.0 epsilon:0.95\n",
      "episode:86 steps:117 mean reward:0.0 epsilon:0.95\n",
      "episode:87 steps:111 mean reward:0.0 epsilon:0.95\n",
      "episode:88 steps:112 mean reward:0.0 epsilon:0.95\n",
      "episode:89 steps:116 mean reward:0.0 epsilon:0.95\n",
      "episode:90 steps:109 mean reward:0.0 epsilon:0.95\n",
      "episode:91 steps:122 mean reward:0.0 epsilon:0.95\n",
      "episode:92 steps:111 mean reward:0.0 epsilon:0.95\n",
      "episode:93 steps:115 mean reward:0.0 epsilon:0.95\n",
      "episode:94 steps:136 mean reward:0.0 epsilon:0.95\n",
      "episode:95 steps:91 mean reward:0.0 epsilon:0.95\n",
      "episode:96 steps:117 mean reward:0.0 epsilon:0.95\n",
      "episode:97 steps:114 mean reward:0.0 epsilon:0.95\n",
      "episode:98 steps:182 mean reward:0.0 epsilon:0.95\n",
      "episode:99 steps:118 mean reward:0.0 epsilon:0.95\n",
      "game over\n"
     ]
    }
   ],
   "source": [
    "step_result = []\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "RL = DeepQLearning( n_actions = 3, \n",
    "                    n_states = 2,\n",
    "                    gamma = 0.99,\n",
    "                    epsilon = 0.95,\n",
    "                    epsilon_increase = 0.001,\n",
    "                    learning_rate = 0.01,\n",
    "                    memory_size = 10000,\n",
    "                    batch_size = 32,\n",
    "                    neuron_num = 10)\n",
    "\n",
    "step_record = training(save_model = True, model_name='mountain_car_deepq')\n",
    "step_result.append(pd.DataFrame(step_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot\n",
    "Steps越大代表每一回合將小車推上山丘的步數越多，因此用步數來評估agent是否有學到正確決策方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7DEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fb60b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step_result = pd.concat(step_result)\n",
    "step_result.columns = ['DeepQ_Agent']\n",
    "step_result.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
